{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sequence Tagging with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import ner_data_utils\n",
    "import ner_model_utils\n",
    "import SequenceTagger\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GermanEval 2014 Dataset\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data and getting it in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded: /Users/thomas/Jupyter_Notebooks/SBWL - Project/data/wikipedia/NER-de-train.tsv !\n",
      "File loaded: /Users/thomas/Jupyter_Notebooks/SBWL - Project/data/wikipedia/NER-de-test.tsv !\n"
     ]
    }
   ],
   "source": [
    "train_file_path = '/Users/thomas/Jupyter_Notebooks/SBWL - Project/data/wikipedia/NER-de-train.tsv'\n",
    "test_file_path = '/Users/thomas/Jupyter_Notebooks/SBWL - Project/data/wikipedia/NER-de-test.tsv'\n",
    "\n",
    "text_as_list, words, unique_chars = ner_data_utils.preprocess_data(train_file_path, False, True)\n",
    "test_text_as_list, test_words, test_unique_chars = ner_data_utils.preprocess_data(test_file_path, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 408728),\n",
       " ('B-LOC', 11585),\n",
       " ('B-PER', 7906),\n",
       " ('B-ORG', 6047),\n",
       " ('I-PER', 4425),\n",
       " ('I-ORG', 3753),\n",
       " ('I-OTH', 3493),\n",
       " ('B-OTH', 3413),\n",
       " ('I-LOC', 1181)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O - entity is dominating. this maked training harder\n",
    "ents = [sent[-1] for sentence in text_as_list for sent in sentence]\n",
    "Counter(ents).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((['S', 'c', 'h', 'a', 'r', 't', 'a', 'u'], 'Schartau'), 'B-PER'),\n",
       " ((['s', 'a', 'g', 't', 'e'], 'sagte'), 'O'),\n",
       " ((['d', 'e', 'm'], 'dem'), 'O'),\n",
       " (([\"'\", \"'\"], \"''\"), 'O'),\n",
       " ((['T', 'a', 'g', 'e', 's', 's', 'p', 'i', 'e', 'g', 'e', 'l'],\n",
       "   'Tagesspiegel'),\n",
       "  'B-ORG'),\n",
       " (([\"'\", \"'\"], \"''\"), 'O'),\n",
       " ((['v', 'o', 'm'], 'vom'), 'O'),\n",
       " ((['F', 'r', 'e', 'i', 't', 'a', 'g'], 'Freitag'), 'O'),\n",
       " (([','], ','), 'O'),\n",
       " ((['F', 'i', 's', 'c', 'h', 'e', 'r'], 'Fischer'), 'B-PER'),\n",
       " ((['s', 'e', 'i'], 'sei'), 'O'),\n",
       " (([\"'\", \"'\"], \"''\"), 'O'),\n",
       " ((['i', 'n'], 'in'), 'O'),\n",
       " ((['e', 'i', 'n', 'e', 'r'], 'einer'), 'O'),\n",
       " ((['W', 'e', 'i', 's', 'e'], 'Weise'), 'O'),\n",
       " ((['a', 'u', 'f', 'g', 'e', 't', 'r', 'e', 't', 'e', 'n'], 'aufgetreten'),\n",
       "  'O'),\n",
       " (([','], ','), 'O'),\n",
       " ((['d', 'i', 'e'], 'die'), 'O'),\n",
       " ((['a', 'l', 'l', 'e', 's'], 'alles'), 'O'),\n",
       " ((['a', 'n', 'd', 'e', 'r', 'e'], 'andere'), 'O'),\n",
       " ((['a', 'l', 's'], 'als'), 'O'),\n",
       " ((['ü', 'b', 'e', 'r', 'z', 'e', 'u', 'g', 'e', 'n', 'd'], 'überzeugend'),\n",
       "  'O'),\n",
       " ((['w', 'a', 'r'], 'war'), 'O'),\n",
       " (([\"'\", \"'\"], \"''\"), 'O'),\n",
       " ((['.'], '.'), 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((['1', '9', '5', '1'], '1951'), 'O'),\n",
       " ((['b', 'i', 's'], 'bis'), 'O'),\n",
       " ((['1', '9', '5', '3'], '1953'), 'O'),\n",
       " ((['w', 'u', 'r', 'd', 'e'], 'wurde'), 'O'),\n",
       " ((['d', 'e', 'r'], 'der'), 'O'),\n",
       " ((['n', 'ö', 'r', 'd', 'l', 'i', 'c', 'h', 'e'], 'nördliche'), 'O'),\n",
       " ((['T', 'e', 'i', 'l'], 'Teil'), 'O'),\n",
       " ((['a', 'l', 's'], 'als'), 'O'),\n",
       " ((['J', 'u', 'g', 'e', 'n', 'd', 'b', 'u', 'r', 'g'], 'Jugendburg'), 'O'),\n",
       " ((['d', 'e', 's'], 'des'), 'O'),\n",
       " ((['K', 'o', 'l', 'p', 'i', 'n', 'g', 'w', 'e', 'r', 'k', 'e', 's'],\n",
       "   'Kolpingwerkes'),\n",
       "  'B-OTH'),\n",
       " ((['g', 'e', 'b', 'a', 'u', 't'], 'gebaut'), 'O'),\n",
       " ((['.'], '.'), 'O')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_as_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the list of words and get them ordered\n",
    "words = np.concatenate((words, test_words), axis = 0)\n",
    "most_common = Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lookup dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = ['B-OTH', 'I-OTH', 'B-LOC', 'I-LOC', 'B-ORG','I-ORG','B-PER', 'I-PER', 'O','<PAD>', '<UNK>']\n",
    "\n",
    "word2ind, ind2word, vocab_size = ner_data_utils.create_lookup_dicts(most_common, specials=specials)\n",
    "char2ind, ind2char, char_vocab_size = ner_data_utils.create_char_lookup_dicts(sorted(list(unique_chars)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84827, 295)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 71382 --> vocab_size with only train sentences\n",
    "# 81720 --> with both, training and testing sentences\n",
    "# 84835 --> both, not lower case. \n",
    "vocab_size , char_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-OTH 0\t\tB-OTH 0\n",
      "I-OTH 1\t\tI-OTH 1\n",
      "B-LOC 2\t\tB-LOC 2\n",
      "I-LOC 3\t\tI-LOC 3\n",
      "B-ORG 4\t\tB-ORG 4\n",
      "I-ORG 5\t\tI-ORG 5\n",
      "B-PER 6\t\tB-PER 6\n",
      "I-PER 7\t\tI-PER 7\n",
      "O 8\t\tO 8\n",
      "<PAD> 9\t\t<PAD> 9\n",
      "<UNK> 10\t\t<UNK> 10\n",
      ". 11\t\t. 11\n",
      ", 12\t\t, 12\n",
      "der 13\t\tder 13\n",
      "die 14\t\tdie 14\n"
     ]
    }
   ],
   "source": [
    "# seems alright\n",
    "count = 0\n",
    "for (k,i), (ii,kk) in zip(word2ind.items(), ind2word.items()):\n",
    "    count +=1\n",
    "    if count > 15:\n",
    "        break\n",
    "    print(k, i, end = '\\t\\t'), print(kk, ii)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_inputs, converted_targets, unk_words, n_unks = ner_data_utils.convert_inputs_and_targets(text_as_list,\n",
    "                                                                                                   word2ind,\n",
    "                                                                                                   test_text_as_list,\n",
    "                                                                                                   char2ind=char2ind,\n",
    "                                                                                                   chars=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(([48, 60, 65, 58, 75, 77, 58, 78], 25830), 6),\n",
       " (([76, 58, 64, 77, 62], 116), 8),\n",
       " (([61, 62, 70], 30), 8),\n",
       " (([6, 6], 19), 8),\n",
       " (([49, 58, 64, 62, 76, 76, 73, 66, 62, 64, 62, 69], 6643), 4),\n",
       " (([6, 6], 19), 8),\n",
       " (([79, 72, 70], 81), 8),\n",
       " (([35, 75, 62, 66, 77, 58, 64], 581), 8),\n",
       " (([11], 12), 8),\n",
       " (([35, 66, 76, 60, 65, 62, 75], 1515), 6),\n",
       " (([76, 62, 66], 114), 8),\n",
       " (([6, 6], 19), 8),\n",
       " (([66, 71], 16), 8),\n",
       " (([62, 66, 71, 62, 75], 55), 8),\n",
       " (([52, 62, 66, 76, 62], 806), 8),\n",
       " (([58, 78, 63, 64, 62, 77, 75, 62, 77, 62, 71], 7702), 8),\n",
       " (([11], 12), 8),\n",
       " (([61, 66, 62], 14), 8),\n",
       " (([58, 69, 69, 62, 76], 290), 8),\n",
       " (([58, 71, 61, 62, 75, 62], 189), 8),\n",
       " (([58, 69, 76], 35), 8),\n",
       " (([143, 59, 62, 75, 83, 62, 78, 64, 62, 71, 61], 11547), 8),\n",
       " (([80, 58, 75], 51), 8),\n",
       " (([6, 6], 19), 8),\n",
       " (([13], 11), 8)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'c', 'h', 'a', 'r', 't', 'a', 'u'] Schartau B-PER\n",
      "['s', 'a', 'g', 't', 'e'] sagte O\n",
      "['d', 'e', 'm'] dem O\n",
      "[\"'\", \"'\"] '' O\n",
      "['T', 'a', 'g', 'e', 's', 's', 'p', 'i', 'e', 'g', 'e', 'l'] Tagesspiegel B-ORG\n",
      "[\"'\", \"'\"] '' O\n",
      "['v', 'o', 'm'] vom O\n",
      "['F', 'r', 'e', 'i', 't', 'a', 'g'] Freitag O\n",
      "[','] , O\n",
      "['F', 'i', 's', 'c', 'h', 'e', 'r'] Fischer B-PER\n",
      "['s', 'e', 'i'] sei O\n",
      "[\"'\", \"'\"] '' O\n",
      "['i', 'n'] in O\n",
      "['e', 'i', 'n', 'e', 'r'] einer O\n",
      "['W', 'e', 'i', 's', 'e'] Weise O\n",
      "['a', 'u', 'f', 'g', 'e', 't', 'r', 'e', 't', 'e', 'n'] aufgetreten O\n",
      "[','] , O\n",
      "['d', 'i', 'e'] die O\n",
      "['a', 'l', 'l', 'e', 's'] alles O\n",
      "['a', 'n', 'd', 'e', 'r', 'e'] andere O\n",
      "['a', 'l', 's'] als O\n",
      "['ü', 'b', 'e', 'r', 'z', 'e', 'u', 'g', 'e', 'n', 'd'] überzeugend O\n",
      "['w', 'a', 'r'] war O\n",
      "[\"'\", \"'\"] '' O\n",
      "['.'] . O\n"
     ]
    }
   ],
   "source": [
    "# it seems to work well. \n",
    "for (word, ent) in converted_inputs[0]:\n",
    "    print([ind2char[ch] for ch in word[0]],ind2word[word[1]], ind2word[ent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/lg/w88j2v8x1x33jfgh34ltmljw0000gn/T/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/nnlm-de-dim128-with-normalization/1'.\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/nnlm-de-dim128-with-normalization/1'.\n",
      "INFO:tensorflow:Initialize variable module/embeddings/part_0:0 from checkpoint b'/var/folders/lg/w88j2v8x1x33jfgh34ltmljw0000gn/T/tfhub_modules/92b9fb774490e712dd3427a83d7dd17b11786803/variables/variables' with embeddings\n"
     ]
    }
   ],
   "source": [
    "embed = hub.Module(\"https://tfhub.dev/google/nnlm-de-dim128-with-normalization/1\")\n",
    "emb = embed([key for key in word2ind.keys()])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    embedding = sess.run(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84827, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./embeddings/my_embedding_tfhub.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now use our class. First we have to create and instance (tagger), and give it the lookup dicts and save and restore paths.\n",
    "Then we can call .build_graph() and then train the model by calling .train() and giving it the train and testing data. \n",
    "In every epoch we print out the models loss score on the training data. \n",
    "In every fifth epoch of the training process we print out the accs and classification reports on the testing data.\n",
    "The trained model will then be saved to save_path. \n",
    "\n",
    "From there we can easily restore the trained model and run evuations, without retraining the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14034"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this creates an input dataset that only uses sentences that contain at least one entity apart from 'O'\n",
    "# this might force the model to learn the differentiation better. \n",
    "ents = ['B-OTH', 'I-OTH', 'B-LOC', 'I-LOC', 'B-ORG','I-ORG','B-PER', 'I-PER']\n",
    "ents_inds = [word2ind[ent] for ent in ents]\n",
    "\n",
    "converted_inputs_new = []\n",
    "\n",
    "for i, sentence in enumerate(converted_inputs):\n",
    "    sent_entities = [sent[1] for sent in sentence]\n",
    "    for ent in ents_inds:\n",
    "        if ent in sent_entities:\n",
    "            converted_inputs_new.append(sentence)\n",
    "            break\n",
    "            \n",
    "# BUT those are 10000 less sentences than before.\n",
    "# seems to enahance performance.\n",
    "len(converted_inputs_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "word2ind = word2ind\n",
    "ind2word = ind2word\n",
    "char2ind = char2ind\n",
    "\n",
    "save_path = './models/sequence_tagger_1/my_model'\n",
    "np_embedding_matrix_path = './embeddings/my_embedding_tfhub.npy'\n",
    "summary_dir = './models/tensorboard/sequence_tagger_1'\n",
    "\n",
    "num_layers = 1\n",
    "n_tags = 10\n",
    "batch_size = 128\n",
    "\n",
    "embedding_dim = 300\n",
    "char_embedding_dim = 50\n",
    "rnn_size = 150\n",
    "char_rnn_size = 40\n",
    "clip = 5\n",
    "\n",
    "learning_rate=0.0001\n",
    "learning_rate_decay_steps=400\n",
    "max_lr = 0.005\n",
    "\n",
    "use_chars = True\n",
    "use_crf = True\n",
    "use_cyclic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_utils.reset_default_graph()\n",
    "tagger = SequenceTagger.SequenceTagger(word2ind,\n",
    "                                       ind2word,\n",
    "                                       save_path = save_path,\n",
    "                                       np_embedding_matrix_path = np_embedding_matrix_path,\n",
    "                                       summary_dir=summary_dir,\n",
    "                                       char2ind = char2ind,\n",
    "                                       use_chars = use_chars,\n",
    "                                       rnn_size = rnn_size,\n",
    "                                       char_rnn_size = char_rnn_size,\n",
    "                                       embedding_dim = embedding_dim,\n",
    "                                       char_embedding_dim = char_embedding_dim,\n",
    "                                       n_tags = n_tags,\n",
    "                                       batch_size = batch_size,\n",
    "                                       use_crf = use_crf,\n",
    "                                       num_layers = num_layers,\n",
    "                                       clip = clip,\n",
    "                                       learning_rate=learning_rate,\n",
    "                                       learning_rate_decay_steps=learning_rate_decay_steps,\n",
    "                                       max_lr=max_lr,\n",
    "                                       use_cyclic=use_cyclic)\n",
    "\n",
    "\n",
    "tagger.build_graph()\n",
    "tagger.train(converted_inputs, converted_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "word2ind = word2ind\n",
    "ind2word = ind2word\n",
    "char2ind = char2ind\n",
    "np_embedding_matrix_path = './embeddings/my_embedding_tfhub.npy'\n",
    "restore_path = './models/sequence_tagger/my_model'\n",
    "\n",
    "num_layers = 1\n",
    "n_tags = 10\n",
    "batch_size = 128\n",
    "\n",
    "embedding_dim = 300\n",
    "char_embedding_dim = 50\n",
    "rnn_size = 150\n",
    "char_rnn_size = 40\n",
    "clip = 5\n",
    "\n",
    "use_chars = True\n",
    "use_crf = True\n",
    "use_cyclic=True\n",
    "train_embeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings from: ./embeddings/my_embedding_tfhub.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built.\n",
      "INFO:tensorflow:Restoring parameters from ./models/sequence_tagger/my_model\n",
      "Restored from ./models/sequence_tagger/my_model\n",
      "Accuracy: 0.94897\n",
      "Accuracy without \"O\"s:0.7645742419156316\n",
      "Number of \"O\"s in data: 87452\n",
      "nNumber of other entitites in data: 8937\n",
      "--> 90.728\n",
      "Classifcation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.58      0.48       773\n",
      "          1       0.49      0.47      0.48       858\n",
      "          2       0.69      0.87      0.77      2385\n",
      "          3       0.62      0.63      0.62       307\n",
      "          4       0.62      0.74      0.68      1324\n",
      "          5       0.70      0.59      0.64       694\n",
      "          6       0.61      0.88      0.72      1693\n",
      "          7       0.91      0.92      0.92       903\n",
      "          8       0.99      0.97      0.98     87452\n",
      "\n",
      "avg / total       0.96      0.95      0.95     96389\n",
      "\n",
      "\n",
      "Classification Report without zeros: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.58      0.66       773\n",
      "          1       0.82      0.47      0.60       858\n",
      "          2       0.88      0.87      0.87      2385\n",
      "          3       0.75      0.63      0.68       307\n",
      "          4       0.86      0.74      0.80      1324\n",
      "          5       0.81      0.59      0.68       694\n",
      "          6       0.87      0.88      0.88      1693\n",
      "          7       0.93      0.92      0.93       903\n",
      "          8       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.86      0.76      0.80      8937\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "ner_model_utils.reset_default_graph()\n",
    "tagger = SequenceTagger.SequenceTagger(word2ind,\n",
    "                                        ind2word,\n",
    "                                        char2ind=char2ind,\n",
    "                                        np_embedding_matrix_path = np_embedding_matrix_path,\n",
    "                                        restore_path=restore_path,\n",
    "                                        use_chars=use_chars,\n",
    "                                        rnn_size=rnn_size,\n",
    "                                        char_rnn_size=char_rnn_size,\n",
    "                                        embedding_dim=embedding_dim,\n",
    "                                        char_embedding_dim=char_embedding_dim,\n",
    "                                        n_tags=n_tags,\n",
    "                                        batch_size=batch_size,\n",
    "                                        keep_probability_i=1.0,\n",
    "                                        keep_probability_o=1.0,\n",
    "                                        keep_probability_h=1.0,\n",
    "                                        keep_probability_d=1.0,\n",
    "                                        keep_probability_e=1.0,\n",
    "                                        use_crf=use_crf,\n",
    "                                        num_layers=num_layers,\n",
    "                                        train_embeddings=train_embeddings)\n",
    "\n",
    "tagger.build_graph()\n",
    "\n",
    "actuals, preds, accuracy, accuracy_without, n_zeros, n_other_ents, classif_report, classif_report_without = tagger.run_evaluate(\n",
    "    converted_targets,\n",
    "    restore_sess=True)\n",
    "print('Accuracy: {:.5f}\\nAccuracy without \"O\"s:{}\\nNumber of \"O\"s in data: {}\\n' \\\n",
    "      'nNumber of other entitites in data: {}\\n--> {:.3f}\\n' \\\n",
    "      'Classifcation Report:\\n {}\\n\\nClassification Report without zeros: \\n{}'.format(accuracy,\n",
    "                                                                                       accuracy_without,\n",
    "                                                                                       n_zeros,\n",
    "                                                                                       n_other_ents,\n",
    "                                                                                       n_zeros / (\n",
    "                                                                                           n_zeros + n_other_ents) * 100,\n",
    "                                                                                       classif_report,\n",
    "                                                                                       classif_report_without))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  445    20    51     1    73     5    36     1   141]\n",
      " [   28   403    19     0     5    31    28    16   328]\n",
      " [   21     5  2073    28    37    29    69     2   121]\n",
      " [    1     8    23   192     0    15     4    12    52]\n",
      " [   47     6   104     1   979    10    53     0   124]\n",
      " [    3    31    29    26    15   411    10    19   150]\n",
      " [   29     8    51     4    26     4  1495    12    64]\n",
      " [    1    10     5     3     1     4    17   835    27]\n",
      " [  514   337   649    53   440    80   722    20 84637]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix \n",
    "print(sklearn.metrics.confusion_matrix(actuals, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentence n.0:\n",
      "['1951', 'bis', '1953', 'wurde', 'der', 'nördliche', 'Teil', 'als', 'Jugendburg', 'des', 'Kolpingwerkes', 'gebaut', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTH', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.1:\n",
      "['Da', 'Muck', 'das', 'Kriegsschreiben', 'nicht', 'überbracht', 'hat', ',', 'wird', 'er', 'als', 'Retter', 'des', 'Landes', 'ausgezeichnet', 'und', 'soll', 'zum', 'Schatzmeister', 'ernannt', 'werden', '.']\n",
      "Actual entities:\n",
      "['O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.2:\n",
      "['Mit', 'Jänner', '2007', 'wurde', 'Robert', 'Schörgenhofer', ',', 'als', 'Nachfolger', 'des', 'ausgeschiedenen', 'Dietmar', 'Drabek', ',', 'in', 'die', 'Kaderliste', 'der', 'FIFA-Schiedsrichter', 'aufgenommen', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.3:\n",
      "['Die', 'These', ',', 'Schlatter', 'sei', 'Antisemit', 'gewesen', ',', 'wurde', 'seither', 'in', 'der', 'theologischen', 'Fachliteratur', 'nicht', 'mehr', 'vertreten', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.4:\n",
      "[\"''\", 'Lehmbruck', '-', 'Beuys', '.Zeichnungen', \"''\", 'lautet', 'der', 'Titel', 'der', 'gerade', 'eröffneten', 'Ausstellung', ',', 'die', 'Kuratorin', 'Dr', '.', 'Marion', 'Bornscheuer', 'bis', 'zum', 'Januar', 'im', 'Lehmbruck-Museum', 'präsentiert', '.']\n",
      "Actual entities:\n",
      "['O', 'B-OTH', 'I-OTH', 'I-OTH', 'I-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.5:\n",
      "['Der', 'Videohoster', 'und', 'die', 'Vertreter', 'der', 'Autoren', 'schieben', 'sich', 'gegenseitig', 'den', 'schwarzen', 'Peter', 'zu', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.6:\n",
      "['Die', 'Kanzel', 'befindet', 'sich', 'an', 'der', 'Südseite', 'des', 'Saals', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.7:\n",
      "['Zuletzt', 'war', 'es', 'Wolfram', 'Graf-Rudolf', ',', 'Chef', 'des', 'Zoos', 'in', 'Aachen', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-ORG', 'O', 'B-LOC', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.8:\n",
      "['Nach', 'den', 'Symptomen', 'lassen', 'sich', 'die', 'beiden', 'Krankheiten', '-', \"''\", 'normale', \"''\", 'Influenza', 'und', 'Schweinegrippe', '-', 'nicht', 'unterscheiden', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.9:\n",
      "['Politisch', 'interessiert', 'ging', 'Antommarchi', 'nach', 'Polen', 'und', 'übernahm', 'dort', '1830', 'während', 'des', 'Novemberaufstandes', 'in', 'Warschau', 'die', 'Leitung', 'der', 'ärztlichen', 'Anstalten', ';', 'kehrte', 'jedoch', 'bald', 'nach', 'Paris', 'zurück', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'B-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.10:\n",
      "['Am', 'Januar', '1839', 'gelang', 'Manuel', 'Bulnes', 'bei', 'der', 'Belagerung', 'von', 'Yungay', 'der', 'entscheidende', 'Sieg', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.11:\n",
      "['Das', 'Auto', 'geriet', 'ins', 'Schleudern', ',', 'kam', 'nach', 'rechts', 'von', 'der', 'Fahrbahn', 'ab', 'und', 'prallte', 'gegen', 'einen', 'Baum', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.12:\n",
      "['Die', 'Dramaturgie', 'der', 'Partie', 'hielt', ',', 'was', 'sich', 'Fußballfans', 'weltweit', 'erträumt', 'hatten', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'B-OTH', 'I-OTH', 'I-OTH', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.13:\n",
      "['DEG-Torhüter', 'Jamie', 'Storr', '(', 'Mitte', ')', 'kann', 'den', 'Schuss', 'zum', '1:0', 'nicht', 'parieren', '.']\n",
      "Actual entities:\n",
      "['B-ORG', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['B-ORG', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.14:\n",
      "['Im', 'Bereich', 'der', 'strukturierten', 'Produkte', 'beobachten', 'wir', 'derzeit', 'sogar', 'Rekordvolumen', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.15:\n",
      "['Die', 'monatelange', 'Führungskrise', 'beim', 'europäischen', 'Luft-und', 'Raumfahrtkonzern', 'EADS', 'ist', 'beigelegt', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-ORG', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-ORG', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.16:\n",
      "['Denn', 'das', 'ist', 'nicht', 'in', 'Ordnung', 'was', 'du', 'heute', 'hier', 'gemacht', 'hast', '!']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.17:\n",
      "['Heute', 'ist', 'durch', 'einen', 'roten', 'Ziegelstreifen', 'der', 'Verlauf', 'des', 'an', 'den', 'Turm', 'anschließenden', 'nordwestlichen', 'Mauerrings', 'gekennzeichnet', ',', 'während', 'der', 'südwestliche', 'Mauerring', 'bis', 'zum', 'Gelände', 'des', 'ehemaligen', 'Franziskanerklosters', 'St', '.', 'Johannis', 'noch', 'intakt', 'ist', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.18:\n",
      "['Verwaltungsdienst', 'Zumeist', 'im', 'Hintergrund', 'handelnd', 'ist', 'der', 'in', 'allen', 'Polizeien', 'tätige', 'Verwaltungsdienst', 'selten', 'in', 'Uniform', 'anzutreffen', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O']\n",
      "\n",
      "\n",
      "\n",
      "Sentence n.19:\n",
      "['Mit', 'Herzog', 'Przemysław', 'von', 'Großpolen', 'schloss', 'Mestwin', 'am', 'Februar', '1282', 'im', 'Vertrag', 'von', 'Kempen', 'eine', '„', 'donatio', 'inter', 'vivos', '“', '(', 'Geschenk', 'unter', 'Lebenden', ')', 'und', 'vermachte', 'ihm', 'sein', 'Herzogtum', '.']\n",
      "Actual entities:\n",
      "['O', 'O', 'B-PER', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted entites:\n",
      "['O', 'O', 'B-PER', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-OTH', 'I-OTH', 'I-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner_data_utils.print_examples(converted_targets[0:20], preds, ind2word, True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
